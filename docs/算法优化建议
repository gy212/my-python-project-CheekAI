你这套管线里（本地 stylometry + 启发式 perplexity + 规则打分 + 聚合/融合）本质上很容易出现“分数离散化/整数化”的现象：

* 你当前的本地段落评分 `score_segment_legacy` 是“阈值命中 + 固定加减分”的**阶梯函数**（比如 +0.18、+0.10…），再加上锚点把分数直接推到 `>=0.72` 或 `<=0.38` 这样的固定区间，天然会导致输出集中在少数几个值上。
* 聚合又是 `overall_probability = Σ(ai_probability * weight) / Σ(weight)` 的加权平均，如果中间任何一步用到**整数除法/整数类型**，会直接把结果“打回整数”。

下面我从“先把整数问题定位清楚”到“如何把算法做成更连续、更可调、更稳健”给你一堆可落地的优化方向（含你提到的 3 点扩展）。

---

## 1) 先定位：到底是哪一层把分数变成“整数”的？

> 这一步非常关键：如果是类型/除法问题，再精细的算法也会被截断成整数。

### A. 特征计算里最常见的坑：**整数除法**

像 `ttr = distinct_tokens / total_tokens`、`repeat_ratio = repeated / total`、`punctuation_ratio = punc / chars` 这种，如果 Rust 里两边是 `usize/u32`，结果会变成 `0` 或 `1`，然后你的规则阈值会被“极端触发”，分数自然就很怪、很离散。
你文档里这些指标全是比例/平均值（ttr、repeat_ratio、ngram_repeat_rate、punctuation_ratio、avg_sentence_len 等）。

**检查要点：**

* 所有比例：分子/分母都先 cast 到 `f64` 再除。
* 所有平均值：`sum as f64 / n as f64`，不要 `sum / n`。

### B. 聚合层最常见的坑：**权重是整数，分子分母没转 float**

你聚合公式是加权平均。
如果实现里写成了：

* `let p = num / den;` 且 `num/den` 是整数类型
* 或者 `ai_probability` 被存成了 `u8/i32`（比如 0~100）

都会导致整体概率“整数化”。

### C. 结构体/序列化层：字段类型写成了整数

比如你的返回结构如果是：

* `ai_probability: u8` / `i32`
* `overall_probability: i64`
  那 JSON 出去就只会有整数。

### D. 前端显示：`Math.round / toFixed(0) / parseInt`

很多项目内部其实是 float，但 UI 显示成：

* `Math.round(probability*100)`
* `toFixed(0)`
  就会看起来“都是整数”。

### E. LLM 返回就是整数

你 LLM 路径里有 JSON 回包（deepseek/glm）。
如果 prompt 要求“给 0~100 的分数（整数）”，那 LLM 会非常乐于只给整数。再加上你本地分数本来就是阶梯型，最终更离散。

> **建议你先做一个小日志：**对每段输出 `ttr/repeat_ratio/ngram_repeat_rate/ppl/avg_sentence_len` + `local_score` + `llm_score` + `aggregated_score`，你马上就能看出是哪一层开始“整数化/离散化”。

---

## 2) 你当前“规则打分”为啥天然离散？怎么改得更连续？

你现在的 `score_segment_legacy` 是典型“硬阈值 + 固定台阶”的阶梯打分：从 `0.5` 开始，命中条件就加/减固定值，最后 clamp。
这种方案的优点是可解释、易调；缺点是：

* 分数会集中在少数点（看起来像“整数百分比”）
* 规则边界处非常敏感（0.579 和 0.581 可能差很多）
* 特征之间的交互很难自然表达

下面是**不引入随机数**、但能显著“连续化”的改法（推荐按优先级做）：

### 方案 1：把“硬阈值”改成“软阈值”（Sigmoid / Smoothstep）

把这类规则：

* `if ttr < 0.58 { score += 0.18 }`
  改成：
* `score += 0.18 * smooth(ttr; center=0.58, width=0.05)`

直觉：ttr 越低，越接近 +0.18；但不是“一刀切”。

可选函数：

* **sigmoid**：`s = 1 / (1 + exp((x - center)/k))`
* **smoothstep**（0~1 的平滑过渡）：适合做“阈值附近渐变”

这样会让分数自然出现小数，而不是只落在几档。

### 方案 2：把“加减分”搬到 log-odds（logit）空间再 sigmoid 回来

目前你在概率空间直接 +0.18，很容易冲到 0.98 clamp。
更稳的方式是：

1. 把 `p` 转成 `logit = ln(p/(1-p))`
2. 在 logit 空间累加各特征贡献
3. 再 `p = sigmoid(logit)`

优点：

* 叠加更自然，不容易“顶死”到 0.98
* 更像统计模型，后续可用数据拟合权重

### 方案 3：锚点（anchors）不要“硬压分数”，改成“强权重项”

你现在强锚点是“满足条件 -> score >= 0.72 + lift”。
这会让大量文本直接落在 0.72~0.9+ 的某些固定区间，离散感非常强。

改法：

* 保留 anchor 条件，但不直接 set/clip score
* 改成给一个**强贡献项**（例如 logit +2.0），并且贡献大小也做连续化（lift 不只跟 ttr，repeat/ppl 也可以一起决定）

### 方案 4：把某些特征改成“连续惩罚曲线”，尤其是句长

你当前句长是分段台阶：`>140` +0.06；`>100` +0.03；`<40` -0.03。
更合理的往往是：

* “过短”惩罚（可能像 AI 也可能像标题/碎片）
* “过长”惩罚（像 AI 长段输出，也可能是论文长句）
* **中间区间最自然**

可以用“两侧 sigmoid”做成 U 型或偏 U 型惩罚，让变化更平滑。

---

## 3) 特征层面：给你一堆“更连续、更鲁棒”的可加特征（不依赖随机数）

你现在已有：ttr、avg_sentence_len、function_word_ratio、repeat_ratio、ngram_repeat_rate、punctuation_ratio、启发式 ppl。
这些是个好起点，但确实会出现“离散 + 误判”问题。下面是可扩展方向：

### A. 词汇丰富度别只用 TTR（TTR 对长度极敏感）

TTR 在短文本上天然偏高，长文本偏低，容易误伤。你可以加：

* **MTLD**（Measure of Textual Lexical Diversity）：对长度更不敏感
* **HD-D**（Hypergeometric Distribution D）
* **Yule’s K / Simpson**：衡量重复/集中度

这些能让“词汇多样性”更连续、更稳定。

### B. Burstiness（“爆发度”）特征：AI 往往更均匀

增加：

* 句长的方差/标准差、变异系数 CV
* 每句 token 数的方差
* 标点间隔的方差（AI 往往节律很均匀）

这类特征很适合解决“看起来很像人写但其实很模板”的情况。

### C. 重复不只看 3-gram：再加“跨句重复”和“长重复片段”

你已有 3-gram 重复率。
可以补：

* distinct-2 / distinct-3（生成式文本常用）
* 句子 embedding 相似度的均值/上分位（语义重复）
* 最长重复子串长度（LRS）/重复片段覆盖率

### D. 功能词比例做“分布”而不是一个 ratio

你现在只有 `function_word_ratio`。
可以做：

* 常见功能词（的、了、是、在、而且、因此…）的**分布熵**
* 功能词 bigram（“因此我们/同时我们/总之…”）的频率
* 连接词密度（AI 常爱用“首先/其次/此外/综上”）

### E. 格式与结构特征（对 LLM 特别有效）

很多 AI 文本的“结构模板”很明显：

* 列表/编号密度（“1. 2. 3.”、“（一）（二）”）
* 标题-正文模式密度（短标题块比例；你已有短标题并入策略，但可以把它当特征）
* 引号/括号密度、冒号后跟长解释的比例
* “总结/结论/综上/总之/注意事项”段落的比例

### F. 中英文分别做 tokenization（否则特征会漂）

你现在分词规则是“英文按词，中文按单字”。
中文按单字会导致：

* TTR 偏高/偏稳定（汉字数量有限、且长度影响方式不同）
* 重复率统计和英文不可比

建议：

* 中文加一层：按词（jieba 或你自己的词典/最大匹配）+ 按字 两套特征并存
* 或者统一用 **字符 n-gram** 作为重复/困惑度的基础，更语言无关

---

## 4) “困惑度 perplexity”这一块：你现在其实是 unigram 熵，更像“词频均匀度”

你现在的 ppl 是用 unigram 熵算的，再做缩放平滑并 clamp 到 `[20, 300]`。
它能反映“词分布是否均匀”，但很难反映“语言模型意义上的可预测性”，所以：

* 对模板化/结构化文本可能有效
* 对高质量 AI 或专业论文风格，可能不稳定

可选增强（按计算成本从低到高）：

### A. 字符级 3~5 gram LM（推荐：性价比高、实现简单、连续）

做一个字符 n-gram 语言模型（带简单平滑），计算 cross-entropy：

* 语言无关（中英文都能跑）
* 对“AI 常见的平滑句式/过度规范标点”更敏感
* 输出天然连续

### B. 词级 2~3 gram（中成本）

需要更可靠分词（尤其中文），但能更贴近“语言流畅度”。

### C. “局部困惑度”：按句/滑窗算，再取分位数

AI 文本往往局部非常平滑、波动小。你可以算：

* ppl 的均值 + 方差
* ppl 的 10/50/90 分位数
  这对 burstiness 很有帮助。

---

## 5) 聚合层（overall_probability）也能优化：让整体结果更稳、更不离散

你现在是按段长度加权平均：`weight = max(50, end-start)`，再算 overall。
可以考虑：

### A. 权重融合置信度，而不只看长度

你本地段落置信度是跟长度相关的：`confidence = min(0.95, 0.55 + min(0.35, len/1800))`。
建议聚合权重改成：

* `weight = len * confidence`
* 或 `weight = sqrt(len) * confidence`（避免长段一票否决）

### B. 用鲁棒聚合替代纯均值（减少“某一段极端值”带崩整体）

可选：

* trimmed mean（去掉最高/最低 10% 段落）
* median-of-means
* 加权中位数（很稳）

这会显著减少“锚点段落把总体顶到某个固定档位”的情况。

### C. 双模式融合别固定 0.6/0.4，可以“按一致性动态调权”

你现在融合是 `0.6*para + 0.4*sent`。
但你也计算了两侧一致性与差异区域。
建议：

* 一致性高：维持 0.6/0.4
* 一致性低：降低整体置信度；同时把权重更多给“更细粒度那一侧”（句子侧），或反过来根据经验决定
* 把 `probability_diff` 用于“置信度折扣”，而不是只用于展示

---

## 6) 你提到的“随机扰动”：完全可以做成**可重复的确定性扰动**

你担心可重复性是对的。但其实可以这样做：

* `seed = hash(normalized_text + algorithm_version)`
* `noise = small_gaussian(seed)` 或 `noise = (hash%1000)/1000 - 0.5`
* `score = clamp(score + noise * 0.01, 0.0, 1.0)`

特点：

* 同一文本、同一算法版本：结果永远一致
* 只在“临界区间”（比如 0.62~0.68）启用扰动，用于打散阶梯感
* 扰动幅度很小（例如 ±0.5~1 个百分点），不改变宏观判断

不过我会把它放在比较后面：**优先用“软阈值/连续化”解决离散问题**，扰动只做最后的“去量化感”。

---

## 7) LLM 参与评分的优化建议（让 LLM 输出也连续、可控）

你现在 LLM 路径是：先本地 `make_segment`，再逐段或批量调用 LLM，失败就回退本地。
这里可以做很多“连续化”和“稳健性”提升：

### A. 强制 LLM 输出小数概率（并固定温度）

* 温度设为 0（或尽量低），减少随机性
* prompt 里明确要求：`0.000~1.000` 的三位小数
* JSON schema 校验：如果不是 float，就当失败回退本地

### B. “门控融合”：LLM 不是覆盖本地，而是按置信度混合

比如：

* 本地 confidence 高（长文本）：更信本地
* 本地 confidence 低（短文本/噪声）：更信 LLM

你已经有 confidence 公式。
可以做：

* `final = α(confidence)*local + (1-α)*llm`
  其中 `α(confidence)` 是连续函数，例如 `α = clamp((confidence-0.55)/0.40, 0, 1)`。

### C. 让 LLM 给“理由向量”而不仅是一个分数

比如让 LLM 也输出（0~1）：

* 重复感
* 模板化结构
* 事实密度/引用密度
* 语气一致性
  然后你把这些作为额外特征进入你本地的连续模型里。

---

## 8) 最后一个大招：用数据把权重“拟合出来”，并做概率校准（强烈推荐）

如果你手头能拿到一点点标注数据（哪怕几百条），你就能把现在这套“手调规则”升级成：

1. 特征仍然用你现有的（ttr、repeat、ngram、ppl、burstiness…）
2. 模型用简单的：

   * Logistic Regression（可解释、连续、部署轻）
   * 或 GBDT（更强，但解释稍弱）
3. 输出再做校准：

   * Platt scaling / Isotonic regression
     让 0.8 真的是“80% 像 AI”的意义，而不是“规则叠加后的一个数字”。

这样做完，“分数全是整数/分档”基本会自然消失。

---

## 一个建议的落地顺序（不折腾、收益最大）

1. **先查整数来源**：特征计算/聚合/序列化/前端显示/LLM 输出格式（上面 1A~1E）
2. **把硬阈值变软阈值**（sigmoid/smoothstep），保留解释体系
3. **锚点改成强贡献项**，不要硬把分数压到某个固定区间
4. **聚合引入置信度权重 + 鲁棒统计**
5. （可选）**确定性扰动**只用于临界区间“去量化”
6. 有数据就上 **logistic + 校准**

---

